Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:45<00:45, 45.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:51<00:00, 22.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:51<00:00, 25.91s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.00it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  2.00it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.74it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.82s/it]
/home/ctisseau/llm_lcd_finetune/ConstraintLM/constraintlm/processors/guide.py:70: UserWarning: Outlines' public *community-contributed* CFG structured generation is experimental. Please review https://dottxt-ai.github.io/outlines/latest/reference/generation/cfg#disclaimer
  warnings.warn(
Epoch 1/3:   0%|          | 0/8 [00:00<?, ?it/s]Epoch 1/3:   0%|          | 0/8 [00:11<?, ?it/s]Epoch 1/3:   0%|          | 0/8 [03:29<?, ?it/s]
Traceback (most recent call last):
  File "/home/ctisseau/llm_lcd_finetune/finetune.py", line 259, in <module>
    finetune(
  File "/home/ctisseau/llm_lcd_finetune/finetune.py", line 200, in finetune
    biased_logits[i, j, :] = cfg.process_logits(batch_input_ids[i, :j].unsqueeze(0), logits[i, j, :].unsqueeze(0)).squeeze(0)  # returns shape (batch, seq_len, vocab)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ctisseau/llm_lcd_finetune/ConstraintLM/constraintlm/processors/structured.py", line 62, in process_logits
    curr_state = self.guide.get_next_state(prev_state, self.tensor_adapter.to_list(gen_ids[-1]))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ctisseau/llm_lcd_finetune/ConstraintLM/constraintlm/processors/guide.py", line 192, in get_next_state
    parser_state = self._get_parser_state_token_applied(state, int(token_id))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ctisseau/llm_lcd_finetune/ConstraintLM/constraintlm/processors/guide.py", line 238, in _get_parser_state_token_applied
    self.parser.parse_from_state(parser_state, is_end=False)
  File "/home/ctisseau/miniconda3/envs/clm_finetune/lib/python3.11/site-packages/outlines/fsm/parsing.py", line 152, in parse_from_state
    return self.parser.parser.parser.parse_from_state(parse_state, is_end=is_end)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ctisseau/miniconda3/envs/clm_finetune/lib/python3.11/site-packages/outlines/fsm/parsing.py", line 536, in parse_from_state
    for token in state.lexer.lex(state):
  File "/home/ctisseau/llm_lcd_finetune/ConstraintLM/constraintlm/processors/guide.py", line 47, in process
    for tok in super().process(stream):
  File "/home/ctisseau/miniconda3/envs/clm_finetune/lib/python3.11/site-packages/lark/indenter.py", line 68, in _process
    assert self.paren_level >= 0
           ^^^^^^^^^^^^^^^^^^^^^
AssertionError
